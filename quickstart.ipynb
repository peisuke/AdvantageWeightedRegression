{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import cuda\n",
    "\n",
    "import chainerrl\n",
    "from chainerrl import replay_buffer\n",
    "from chainerrl.agent import AttributeSavingMixin\n",
    "from chainerrl.misc.batch_states import batch_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict action\n",
    "class Actor(chainer.Chain):\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=50):\n",
    "        self.beta = 1.0\n",
    "        self.min_prob = 0.0\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_actions)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        h = self.l2(h)\n",
    "        return chainerrl.distribution.SoftmaxDistribution(\n",
    "            h, beta=self.beta, min_prob=self.min_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict value\n",
    "class Critic(chainer.Chain):\n",
    "    def __init__(self, obs_size, n_hidden_channels=50):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        r = self.l2(h)\n",
    "        return r.reshape(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_train(chain):\n",
    "    call_orig = chain.__call__\n",
    "\n",
    "    def call_test(self, x):\n",
    "        with chainer.using_config('train', False):\n",
    "            return call_orig(self, x)\n",
    "\n",
    "    chain.__call__ = call_test\n",
    "\n",
    "\n",
    "class AWRModel(chainer.Chain):\n",
    "    def __init__(self, actor, critic):\n",
    "        super().__init__(actor=actor, critic=critic)\n",
    "        \n",
    "\n",
    "class AWR(AttributeSavingMixin):\n",
    "    ADV_EPS = 1e-5\n",
    "        \n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                 model, \n",
    "                 actor_optimizer, \n",
    "                 critic_optimizer, \n",
    "                 replay_buffer,\n",
    "                 gamma=0.95,\n",
    "                 minibatch_size=32,\n",
    "                 gpu=None,\n",
    "                 phi=lambda x: x,\n",
    "                 batch_states=batch_states):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.xp = self.model.xp\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gpu = gpu\n",
    "        self.phi = phi\n",
    "        self.gamma = gamma\n",
    "        self.batch_states = batch_states\n",
    "        self.actor = self.model['actor']\n",
    "        self.critic = self.model['critic']\n",
    "        self.temp = 1.0\n",
    "        self.weight_clip = 20\n",
    "\n",
    "        self.total_actor_loss = 0\n",
    "        self.total_critic_loss = 0\n",
    "        \n",
    "        \n",
    "    def act(self, obs, test=True):\n",
    "        with chainer.using_config('train', not test):\n",
    "            s = self.batch_states([obs], self.xp, self.phi)\n",
    "            r = self.actor(s)\n",
    "            \n",
    "            if test:\n",
    "                a = F.argmax(r.logits, axis=1)\n",
    "            else:\n",
    "                a = r.sample()            \n",
    "\n",
    "        return cuda.to_cpu(a.array[0])\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        return [\n",
    "            ('average_q', self.average_q),\n",
    "            ('average_loss', self.average_loss)\n",
    "        ]\n",
    "    \n",
    "    def stop_episode(self):\n",
    "        self.replay_buffer.stop_current_episode()\n",
    "    \n",
    "    def update(self, n_sample_size=128, errors_out=None):\n",
    "        # Sample episodes\n",
    "        batchsize = self.minibatch_size\n",
    "        n_episodes = self.replay_buffer.n_episodes\n",
    "\n",
    "        if n_sample_size > n_episodes:\n",
    "            n_sample_size = n_episodes\n",
    "\n",
    "        episodes = self.replay_buffer.sample_episodes(n_sample_size)\n",
    "        \n",
    "        # Compute Rewards with td-lambda\n",
    "        data = self.sample_data(episodes)\n",
    "        nb_data = len(data)\n",
    "        perm  = np.random.permutation(nb_data)\n",
    "\n",
    "        # Update critic\n",
    "        self.total_critic_count = 0\n",
    "        self.total_critic_loss = 0\n",
    "        for i in range(0, nb_data, batchsize):\n",
    "            batch_data = [data[idx] for idx in perm[i:i+batchsize]]\n",
    "            states = np.array([d['episode']['state'] for d in batch_data])\n",
    "            new_vals = np.array([d['new_val'] for d in batch_data])\n",
    "\n",
    "            # create batch from states and new_rewards\n",
    "            val = self.critic(states.astype(np.float32))            \n",
    "            critic_loss = F.sum(F.square(val - new_vals))\n",
    "            \n",
    "            self.critic.cleargrads()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.update()\n",
    "    \n",
    "            self.total_critic_count += batchsize\n",
    "            self.total_critic_loss += critic_loss.data\n",
    "        self.total_critic_loss /= self.total_critic_count\n",
    "        \n",
    "\n",
    "        # Re-compute Rewards with td-lambda\n",
    "        data = self.sample_data(episodes)\n",
    "        nb_data = len(data)\n",
    "        perm  = np.random.permutation(nb_data)\n",
    "\n",
    "        # Update actor\n",
    "        self.total_actor_count = 0\n",
    "        self.total_actor_loss = 0\n",
    "        for i in range(0, nb_data, batchsize):\n",
    "            states = np.array([d['episode']['state'] for d in batch_data])\n",
    "            vals = np.array([d['val'] for d in batch_data])\n",
    "            new_vals = np.array([d['new_val'] for d in batch_data])\n",
    "            actions = np.array([d['episode']['action'] for d in batch_data])\n",
    "\n",
    "            # create batch from states and new_rewards\n",
    "            adv = new_vals - vals\n",
    "            #adv = (adv - adv.mean()) / (adv.std() + self.ADV_EPS)\n",
    "            weights = np.exp(adv / self.temp)\n",
    "            weights = np.minimum(weights, self.weight_clip)\n",
    "\n",
    "            val = self.actor(states.astype(np.float32))\n",
    "            actor_loss = -F.sum(val.log_prob(actions) * weights) \n",
    "            \n",
    "            self.actor.cleargrads()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.update()\n",
    "                        \n",
    "            self.total_actor_count += batchsize\n",
    "            self.total_actor_loss += actor_loss.data\n",
    "        self.total_actor_loss /= self.total_actor_count\n",
    "        \n",
    "    def rollout_path(self, test=True, max_length=None):\n",
    "        obs = self.env.reset()\n",
    "        done = False\n",
    "        R = 0\n",
    "        t = 0\n",
    "        \n",
    "        path = []\n",
    "        while not done:\n",
    "            if max_length is not None and t >= max_length:\n",
    "                break\n",
    "                \n",
    "            action = self.act(obs, test)\n",
    "            new_obs, r, done, _ = env.step(action)            \n",
    "            path.append({\n",
    "                'state': obs,\n",
    "                'reward': r,\n",
    "                'action': action,\n",
    "                'next_state': new_obs,\n",
    "                'is_state_terminal': done\n",
    "            })            \n",
    "            R += r\n",
    "            t += 1            \n",
    "            obs = new_obs\n",
    "        \n",
    "        return path, R\n",
    "    \n",
    "    def rollout_train(self, num_samples, max_length=None):\n",
    "        new_sample_count = 0\n",
    "        total_return = 0\n",
    "        path_count = 0\n",
    "        \n",
    "        while (new_sample_count < num_samples):\n",
    "            path, path_return = self.rollout_path(test=False, max_length=max_length)                        \n",
    "            self.store_path(path)\n",
    "            \n",
    "            new_sample_count += len(path)\n",
    "            total_return += path_return\n",
    "            path_count += 1\n",
    "\n",
    "        avg_return = total_return / path_count\n",
    "        return avg_return, path_count, new_sample_count\n",
    "    \n",
    "    def rollout_test(self, num_episodes, max_length=None, print_info=False):\n",
    "        total_return = 0\n",
    "        for e in range(num_episodes):\n",
    "            path, path_return = self.rollout_path(test=True, max_length=max_length)\n",
    "            total_return += path_return\n",
    "\n",
    "            if (print_info):\n",
    "                print(\"Episode: {:d}\".format(e))\n",
    "                print(\"Curr_Return: {:.3f}\".format(path_return))\n",
    "                print(\"Avg_Return: {:.3f}\\n\".format(total_return / (e + 1)))\n",
    "\n",
    "        avg_return = total_return / num_episodes\n",
    "        return avg_return, num_episodes    \n",
    "    \n",
    "    def stop_episode(self):\n",
    "        self.replay_buffer.stop_current_episode()\n",
    "        \n",
    "    def store_path(self, path):\n",
    "        for p in path:\n",
    "            self.replay_buffer.append(**p)\n",
    "        self.stop_episode()\n",
    "            \n",
    "    def get_total_samples(self):\n",
    "        return len(self.replay_buffer)     \n",
    "    \n",
    "    def compute_return(self, episode, val_t, td_lambda=0.9):\n",
    "        path_len = len(episode)\n",
    "\n",
    "        return_t = np.zeros(path_len)\n",
    "        last_val = episode[-1]['reward']\n",
    "        return_t[-1] = last_val\n",
    "\n",
    "        for i in reversed(range(0, path_len - 1)):    \n",
    "            curr_r = episode[i]['reward']\n",
    "            next_ret = return_t[i+1]\n",
    "            curr_val = curr_r + self.gamma * ((1.0 - td_lambda) * val_t[i+1] + td_lambda * next_ret)\n",
    "            return_t[i] = curr_val\n",
    "\n",
    "        return return_t\n",
    "    \n",
    "    def sample_data(self, episodes):\n",
    "        vals = []\n",
    "        new_vals = []\n",
    "\n",
    "        for episode in episodes:\n",
    "            states = np.array([e['state'] for e in episode])\n",
    "            val_t = self.critic(states.astype(np.float32))\n",
    "            val_t = val_t.data\n",
    "            new_val_t = self.compute_return(episode, val_t) \n",
    "\n",
    "            vals.append(val_t)\n",
    "            new_vals.append(new_val_t)\n",
    "\n",
    "        data = []\n",
    "        for episode, val, new_val in zip(episodes, vals, new_vals):\n",
    "            for e, v, nv in zip(episode, val, new_val):\n",
    "                data.append({'episode': e, 'val': v, 'new_val': nv})\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(obs_size, n_actions)\n",
    "critic = Critic(obs_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AWRModel(actor=actor, critic=critic)\n",
    "\n",
    "opt_a = chainer.optimizers.SGD(lr=0.00005)\n",
    "opt_c = chainer.optimizers.SGD(lr=0.001)\n",
    "opt_a.setup(actor)\n",
    "opt_c.setup(critic)\n",
    "\n",
    "rbuf = replay_buffer.EpisodicReplayBuffer(10 ** 6)\n",
    "phi = lambda x: x.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_iter=256\n",
    "max_episode_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr:  0 (8.0, 1)\n",
      "itr:  1 (75.0, 1)\n",
      "itr:  2 (125.0, 1)\n",
      "itr:  3 (200.0, 1)\n",
      "itr:  4 (155.0, 1)\n",
      "itr:  5 (109.0, 1)\n",
      "itr:  6 (109.0, 1)\n",
      "itr:  7 (126.0, 1)\n",
      "itr:  8 (126.0, 1)\n",
      "itr:  9 (200.0, 1)\n"
     ]
    }
   ],
   "source": [
    "awr = AWR(env, model, opt_a, opt_c, rbuf, phi=phi)\n",
    "\n",
    "for itr in range(10):\n",
    "    for i in range(10):\n",
    "        awr.rollout_train(samples_per_iter, max_episode_length)\n",
    "        awr.update(n_sample_size=128)    \n",
    "    ret = awr.rollout_test(1, max_episode_length, print_info=False)\n",
    "    print('itr: ', itr, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test episode: 0 R: 200.0\n",
      "test episode: 1 R: 200.0\n",
      "test episode: 2 R: 200.0\n",
      "test episode: 3 R: 200.0\n",
      "test episode: 4 R: 200.0\n",
      "test episode: 5 R: 200.0\n",
      "test episode: 6 R: 200.0\n",
      "test episode: 7 R: 200.0\n",
      "test episode: 8 R: 200.0\n",
      "test episode: 9 R: 200.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while not done and t < 200:\n",
    "        env.render()\n",
    "        action = awr.act(obs)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        R += r\n",
    "        t += 1\n",
    "    print('test episode:', i, 'R:', R)\n",
    "    awr.stop_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
